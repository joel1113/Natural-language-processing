{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dreE5BQSQLTg",
        "outputId": "229c504a-da46-41d8-819f-c3d342187b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shapes:\n",
            "English data shape: (7, 4)\n",
            "French data shape: (7, 5)\n",
            "\n",
            "Training data shapes:\n",
            "Decoder input shape: (7, 4)\n",
            "Decoder target shape: (7, 4)\n",
            "Epoch 1/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 244ms/step - accuracy: 0.1063 - loss: 2.7703 - val_accuracy: 0.2500 - val_loss: 2.7608\n",
            "Epoch 2/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2500 - loss: 2.7517 - val_accuracy: 0.2500 - val_loss: 2.7481\n",
            "Epoch 3/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.2500 - loss: 2.7322 - val_accuracy: 0.2500 - val_loss: 2.7324\n",
            "Epoch 4/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2500 - loss: 2.7162 - val_accuracy: 0.2500 - val_loss: 2.7149\n",
            "Epoch 5/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2500 - loss: 2.6815 - val_accuracy: 0.2500 - val_loss: 2.6866\n",
            "Epoch 6/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2500 - loss: 2.6507 - val_accuracy: 0.2500 - val_loss: 2.6484\n",
            "Epoch 7/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2500 - loss: 2.6097 - val_accuracy: 0.2500 - val_loss: 2.5905\n",
            "Epoch 8/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2500 - loss: 2.5578 - val_accuracy: 0.2500 - val_loss: 2.5061\n",
            "Epoch 9/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.2500 - loss: 2.4360 - val_accuracy: 0.2500 - val_loss: 2.3783\n",
            "Epoch 10/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2500 - loss: 2.2688 - val_accuracy: 0.2500 - val_loss: 2.2606\n",
            "Epoch 11/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2500 - loss: 2.1637 - val_accuracy: 0.2500 - val_loss: 2.2569\n",
            "Epoch 12/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2500 - loss: 2.1041 - val_accuracy: 0.2500 - val_loss: 2.2277\n",
            "Epoch 13/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2500 - loss: 2.0808 - val_accuracy: 0.2500 - val_loss: 2.1332\n",
            "Epoch 14/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2500 - loss: 2.1276 - val_accuracy: 0.5000 - val_loss: 2.0945\n",
            "Epoch 15/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.4344 - loss: 1.8983 - val_accuracy: 0.5000 - val_loss: 2.0820\n",
            "Epoch 16/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3406 - loss: 2.0723 - val_accuracy: 0.5000 - val_loss: 2.0972\n",
            "Epoch 17/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3938 - loss: 1.8394 - val_accuracy: 0.5000 - val_loss: 2.1246\n",
            "Epoch 18/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3531 - loss: 1.6900 - val_accuracy: 0.5000 - val_loss: 2.1755\n",
            "Epoch 19/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4344 - loss: 1.7167 - val_accuracy: 0.5000 - val_loss: 2.2440\n",
            "Epoch 20/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3562 - loss: 1.8352 - val_accuracy: 0.5000 - val_loss: 2.3321\n",
            "Epoch 21/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4844 - loss: 1.7269 - val_accuracy: 0.5000 - val_loss: 2.4258\n",
            "Epoch 22/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5250 - loss: 1.6339 - val_accuracy: 0.3750 - val_loss: 2.5301\n",
            "Epoch 23/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5969 - loss: 1.4858 - val_accuracy: 0.5000 - val_loss: 2.6589\n",
            "Epoch 24/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6219 - loss: 1.5442 - val_accuracy: 0.3750 - val_loss: 2.7753\n",
            "Epoch 25/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6844 - loss: 1.3086 - val_accuracy: 0.3750 - val_loss: 2.9035\n",
            "Epoch 26/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.6219 - loss: 1.3399 - val_accuracy: 0.3750 - val_loss: 3.0595\n",
            "Epoch 27/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6844 - loss: 1.1422 - val_accuracy: 0.3750 - val_loss: 3.2469\n",
            "Epoch 28/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6531 - loss: 1.1155 - val_accuracy: 0.3750 - val_loss: 3.4178\n",
            "Epoch 29/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6219 - loss: 1.0748 - val_accuracy: 0.3750 - val_loss: 3.5667\n",
            "Epoch 30/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6531 - loss: 0.9124 - val_accuracy: 0.3750 - val_loss: 3.7031\n",
            "Epoch 31/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6625 - loss: 0.8787 - val_accuracy: 0.3750 - val_loss: 3.8272\n",
            "Epoch 32/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6938 - loss: 0.7788 - val_accuracy: 0.3750 - val_loss: 3.9468\n",
            "Epoch 33/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8313 - loss: 0.5986 - val_accuracy: 0.3750 - val_loss: 4.0444\n",
            "Epoch 34/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9281 - loss: 0.5834 - val_accuracy: 0.3750 - val_loss: 4.1390\n",
            "Epoch 35/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9594 - loss: 0.5523 - val_accuracy: 0.3750 - val_loss: 4.2100\n",
            "Epoch 36/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.4933 - val_accuracy: 0.3750 - val_loss: 4.2674\n",
            "Epoch 37/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.3550 - val_accuracy: 0.5000 - val_loss: 4.3308\n",
            "Epoch 38/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.3806 - val_accuracy: 0.5000 - val_loss: 4.3705\n",
            "Epoch 39/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.3815 - val_accuracy: 0.5000 - val_loss: 4.4137\n",
            "Epoch 40/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2947 - val_accuracy: 0.5000 - val_loss: 4.4752\n",
            "Epoch 41/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.2767 - val_accuracy: 0.5000 - val_loss: 4.4987\n",
            "Epoch 42/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.2647 - val_accuracy: 0.5000 - val_loss: 4.5310\n",
            "Epoch 43/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2526 - val_accuracy: 0.5000 - val_loss: 4.5716\n",
            "Epoch 44/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1931 - val_accuracy: 0.5000 - val_loss: 4.6009\n",
            "Epoch 45/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.1761 - val_accuracy: 0.5000 - val_loss: 4.6230\n",
            "Epoch 46/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1761 - val_accuracy: 0.5000 - val_loss: 4.6548\n",
            "Epoch 47/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.1552 - val_accuracy: 0.5000 - val_loss: 4.7058\n",
            "Epoch 48/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.1714 - val_accuracy: 0.5000 - val_loss: 4.7341\n",
            "Epoch 49/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1589 - val_accuracy: 0.5000 - val_loss: 4.7486\n",
            "Epoch 50/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.1454 - val_accuracy: 0.5000 - val_loss: 4.7666\n",
            "Epoch 51/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.1425 - val_accuracy: 0.5000 - val_loss: 4.7931\n",
            "Epoch 52/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1276 - val_accuracy: 0.5000 - val_loss: 4.7981\n",
            "Epoch 53/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.1055 - val_accuracy: 0.5000 - val_loss: 4.8122\n",
            "Epoch 54/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1181 - val_accuracy: 0.5000 - val_loss: 4.8307\n",
            "Epoch 55/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.1000 - val_accuracy: 0.5000 - val_loss: 4.8486\n",
            "Epoch 56/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.1021 - val_accuracy: 0.5000 - val_loss: 4.8651\n",
            "Epoch 57/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0951 - val_accuracy: 0.5000 - val_loss: 4.8773\n",
            "Epoch 58/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0962 - val_accuracy: 0.5000 - val_loss: 4.8876\n",
            "Epoch 59/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0756 - val_accuracy: 0.5000 - val_loss: 4.9026\n",
            "Epoch 60/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0833 - val_accuracy: 0.5000 - val_loss: 4.9192\n",
            "Epoch 61/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.0809 - val_accuracy: 0.5000 - val_loss: 4.9358\n",
            "Epoch 62/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0772 - val_accuracy: 0.5000 - val_loss: 4.9499\n",
            "Epoch 63/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0748 - val_accuracy: 0.5000 - val_loss: 4.9613\n",
            "Epoch 64/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0702 - val_accuracy: 0.5000 - val_loss: 4.9759\n",
            "Epoch 65/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0584 - val_accuracy: 0.5000 - val_loss: 4.9902\n",
            "Epoch 66/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0553 - val_accuracy: 0.5000 - val_loss: 5.0013\n",
            "Epoch 67/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0614 - val_accuracy: 0.5000 - val_loss: 5.0137\n",
            "Epoch 68/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0598 - val_accuracy: 0.5000 - val_loss: 5.0254\n",
            "Epoch 69/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0542 - val_accuracy: 0.5000 - val_loss: 5.0362\n",
            "Epoch 70/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0525 - val_accuracy: 0.5000 - val_loss: 5.0451\n",
            "Epoch 71/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0456 - val_accuracy: 0.5000 - val_loss: 5.0571\n",
            "Epoch 72/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0440 - val_accuracy: 0.5000 - val_loss: 5.0696\n",
            "Epoch 73/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0476 - val_accuracy: 0.5000 - val_loss: 5.0787\n",
            "Epoch 74/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0442 - val_accuracy: 0.5000 - val_loss: 5.0881\n",
            "Epoch 75/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0410 - val_accuracy: 0.5000 - val_loss: 5.0989\n",
            "Epoch 76/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.0417 - val_accuracy: 0.5000 - val_loss: 5.1096\n",
            "Epoch 77/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0367 - val_accuracy: 0.5000 - val_loss: 5.1194\n",
            "Epoch 78/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0370 - val_accuracy: 0.5000 - val_loss: 5.1281\n",
            "Epoch 79/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 0.0356 - val_accuracy: 0.5000 - val_loss: 5.1360\n",
            "Epoch 80/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0338 - val_accuracy: 0.5000 - val_loss: 5.1448\n",
            "Epoch 81/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0317 - val_accuracy: 0.5000 - val_loss: 5.1548\n",
            "Epoch 82/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0346 - val_accuracy: 0.5000 - val_loss: 5.1650\n",
            "Epoch 83/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.0332 - val_accuracy: 0.5000 - val_loss: 5.1751\n",
            "Epoch 84/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0309 - val_accuracy: 0.5000 - val_loss: 5.1852\n",
            "Epoch 85/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0288 - val_accuracy: 0.5000 - val_loss: 5.1954\n",
            "Epoch 86/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0299 - val_accuracy: 0.5000 - val_loss: 5.2054\n",
            "Epoch 87/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0290 - val_accuracy: 0.5000 - val_loss: 5.2146\n",
            "Epoch 88/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0269 - val_accuracy: 0.5000 - val_loss: 5.2231\n",
            "Epoch 89/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0265 - val_accuracy: 0.5000 - val_loss: 5.2309\n",
            "Epoch 90/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0247 - val_accuracy: 0.5000 - val_loss: 5.2388\n",
            "Epoch 91/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0259 - val_accuracy: 0.5000 - val_loss: 5.2466\n",
            "Epoch 92/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0253 - val_accuracy: 0.5000 - val_loss: 5.2547\n",
            "Epoch 93/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0235 - val_accuracy: 0.5000 - val_loss: 5.2633\n",
            "Epoch 94/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0233 - val_accuracy: 0.5000 - val_loss: 5.2706\n",
            "Epoch 95/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0224 - val_accuracy: 0.5000 - val_loss: 5.2783\n",
            "Epoch 96/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0213 - val_accuracy: 0.5000 - val_loss: 5.2850\n",
            "Epoch 97/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0211 - val_accuracy: 0.5000 - val_loss: 5.2916\n",
            "Epoch 98/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0210 - val_accuracy: 0.5000 - val_loss: 5.2985\n",
            "Epoch 99/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0201 - val_accuracy: 0.5000 - val_loss: 5.3062\n",
            "Epoch 100/100\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0194 - val_accuracy: 0.5000 - val_loss: 5.3134\n",
            "\n",
            "Translations:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n",
            "English: hello\n",
            "French: bonjour end\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "English: thank you\n",
            "French: comment end\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "English: your name\n",
            "French: comment m'appelle end\n",
            "\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "English: how are you\n",
            "French: comment ça va end\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample dataset\n",
        "data = [\n",
        "    (\"hello\", \"bonjour\"),\n",
        "    (\"how are you\", \"comment ça va\"),\n",
        "    (\"I am fine\", \"je vais bien\"),\n",
        "    (\"what is your name\", \"comment tu t'appelles\"),\n",
        "    (\"my name is\", \"je m'appelle\"),\n",
        "    (\"thank you\", \"merci\"),\n",
        "    (\"goodbye\", \"au revoir\")\n",
        "]\n",
        "\n",
        "# (a) Data Preprocessing\n",
        "def preprocess_data(data):\n",
        "    # Split English and French sentences\n",
        "    eng_texts = [pair[0] for pair in data]\n",
        "    fra_texts = ['<start> ' + pair[1] + ' <end>' for pair in data]\n",
        "\n",
        "    # Create tokenizers\n",
        "    eng_tokenizer = Tokenizer()\n",
        "    fra_tokenizer = Tokenizer()\n",
        "\n",
        "    # Fit tokenizers\n",
        "    eng_tokenizer.fit_on_texts(eng_texts)\n",
        "    fra_tokenizer.fit_on_texts(fra_texts)\n",
        "\n",
        "    # Convert texts to sequences\n",
        "    eng_sequences = eng_tokenizer.texts_to_sequences(eng_texts)\n",
        "    fra_sequences = fra_tokenizer.texts_to_sequences(fra_texts)\n",
        "\n",
        "    # Find maximum lengths\n",
        "    max_eng_len = max(len(seq) for seq in eng_sequences)\n",
        "    max_fra_len = max(len(seq) for seq in fra_sequences)\n",
        "\n",
        "    # Pad sequences\n",
        "    eng_padded = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\n",
        "    fra_padded = pad_sequences(fra_sequences, maxlen=max_fra_len, padding='post')\n",
        "\n",
        "    return eng_padded, fra_padded, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len\n",
        "\n",
        "# Process the data\n",
        "eng_data, fra_data, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len = preprocess_data(data)\n",
        "\n",
        "# Get vocabulary sizes\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
        "\n",
        "# Print shapes for debugging\n",
        "print(\"Input shapes:\")\n",
        "print(f\"English data shape: {eng_data.shape}\")\n",
        "print(f\"French data shape: {fra_data.shape}\")\n",
        "\n",
        "# (b) Build Seq2Seq Model\n",
        "def build_model(input_vocab, output_vocab, input_length, output_length):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(input_length,))\n",
        "    encoder_embedding = Embedding(input_vocab, 50)(encoder_inputs)\n",
        "    encoder = LSTM(100, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(output_length,))\n",
        "    decoder_embedding = Embedding(output_vocab, 50)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(100, return_sequences=True)\n",
        "    decoder_outputs = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(output_vocab, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Create model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model\n",
        "\n",
        "# (c) Prepare Data for Training\n",
        "decoder_input_data = fra_data[:, :-1]  # Remove last token\n",
        "decoder_target_data = fra_data[:, 1:]  # Remove first token\n",
        "\n",
        "# Print shapes for debugging\n",
        "print(\"\\nTraining data shapes:\")\n",
        "print(f\"Decoder input shape: {decoder_input_data.shape}\")\n",
        "print(f\"Decoder target shape: {decoder_target_data.shape}\")\n",
        "\n",
        "# (d) Train the Model\n",
        "model = build_model(\n",
        "    eng_vocab_size,\n",
        "    fra_vocab_size,\n",
        "    max_eng_len,\n",
        "    max_fra_len - 1\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    [eng_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=2,\n",
        "    epochs=100,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# (e) Inference Setup\n",
        "class Translator:\n",
        "    def __init__(self, model, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len):\n",
        "        self.model = model\n",
        "        self.eng_tokenizer = eng_tokenizer\n",
        "        self.fra_tokenizer = fra_tokenizer\n",
        "        self.max_eng_len = max_eng_len\n",
        "        self.max_fra_len = max_fra_len\n",
        "        self.fra_index_word = {v: k for k, v in fra_tokenizer.word_index.items()}\n",
        "\n",
        "    def translate(self, text):\n",
        "        # Tokenize input text\n",
        "        sequence = self.eng_tokenizer.texts_to_sequences([text])\n",
        "        padded = pad_sequences(sequence, maxlen=self.max_eng_len, padding='post')\n",
        "\n",
        "        # Initialize target sequence\n",
        "        target_seq = np.zeros((1, self.max_fra_len - 1))\n",
        "\n",
        "        # Generate translation\n",
        "        prediction = self.model.predict([padded, target_seq])\n",
        "\n",
        "        # Convert prediction to text\n",
        "        output_sequence = np.argmax(prediction[0], axis=1)\n",
        "        translated_text = []\n",
        "\n",
        "        for idx in output_sequence:\n",
        "            if idx != 0:\n",
        "                word = self.fra_index_word.get(idx, '')\n",
        "                if word and word not in ['<start>', '<end>']:\n",
        "                    translated_text.append(word)\n",
        "\n",
        "        return ' '.join(translated_text)\n",
        "\n",
        "# (f) Translate New Sentences\n",
        "translator = Translator(model, eng_tokenizer, fra_tokenizer, max_eng_len, max_fra_len)\n",
        "\n",
        "# Test translations\n",
        "test_sentences = [\n",
        "    \"hello\",\n",
        "    \"thank you\",\n",
        "    \"your name\",\n",
        "    \"how are you\"\n",
        "]\n",
        "\n",
        "print(\"\\nTranslations:\")\n",
        "for sentence in test_sentences:\n",
        "    translation = translator.translate(sentence)\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"French: {translation}\\n\")"
      ]
    }
  ]
}